import React, { useState, useEffect, useRef } from "react";
import { Mic, MicOff } from "lucide-react";
import socketService from "../../services/socketService";

const CommentarySection = ({ isActive = true }) => {
  const [isStreaming, setIsStreaming] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const audioContextRef = useRef(null);
  const mediaStreamSourceRef = useRef(null);
  const scriptNodeRef = useRef(null);
  const streamRef = useRef(null);

  const isSupported =
    typeof navigator !== "undefined" &&
    navigator.mediaDevices &&
    navigator.mediaDevices.getUserMedia &&
    typeof AudioContext !== "undefined";

  useEffect(() => {
    return () => {
      stopAllStreaming();
    };
  }, []);

  useEffect(() => {
    if (!isActive) {
      console.log("üîá [CommentarySection] Tab inactive, stopping streaming");
      stopAllStreaming();
    }
  }, [isActive]);

  const stopAllStreaming = () => {
    // Stop script processor
    if (scriptNodeRef.current) {
      scriptNodeRef.current.disconnect();
      scriptNodeRef.current = null;
    }

    // Disconnect media stream source
    if (mediaStreamSourceRef.current) {
      mediaStreamSourceRef.current.disconnect();
      mediaStreamSourceRef.current = null;
    }

    // Close audio context
    if (audioContextRef.current && audioContextRef.current.state !== "closed") {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    // Stop media stream
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    setIsStreaming(false);
    setIsProcessing(false);
  };

  const setupAudioStream = async (stream) => {
    try {
      // Create audio context with low latency
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
      audioContextRef.current.latencyHint = 'interactive';

      // Create media stream source
      mediaStreamSourceRef.current = audioContextRef.current.createMediaStreamSource(stream);

      console.log("üéôÔ∏è Audio stream setup completed");
      return true;
    } catch (error) {
      console.error("‚ùå Error setting up audio stream:", error);
      return false;
    }
  };

  const startStreaming = () => {
    if (!audioContextRef.current || !mediaStreamSourceRef.current) {
      console.error("‚ùå Audio context or media stream source not available");
      return;
    }

    try {
      const bufferSize = 2048;
      scriptNodeRef.current = audioContextRef.current.createScriptProcessor(bufferSize, 1, 1);

      scriptNodeRef.current.onaudioprocess = (audioProcessingEvent) => {
        if (!isStreaming) return;

        const inputBuffer = audioProcessingEvent.inputBuffer;
        const audioData = inputBuffer.getChannelData(0);

        // Send Float32Array directly to server
        if (socketService.socket && socketService.socket.connected) {
          socketService.sendRefereeVoiceRealtime(Array.from(audioData));
        }
      };

      // Connect nodes
      mediaStreamSourceRef.current.connect(scriptNodeRef.current);
      scriptNodeRef.current.connect(audioContextRef.current.destination);

      console.log("üéôÔ∏è Started real-time audio streaming");
      setIsStreaming(true);
    } catch (error) {
      console.error("‚ùå Error starting streaming:", error);
    }
  };

  const stopStreaming = () => {
    console.log("üîá Stopping real-time streaming");

    if (scriptNodeRef.current) {
      scriptNodeRef.current.disconnect();
      scriptNodeRef.current = null;
    }

    setIsStreaming(false);
  };

  const startRecording = async () => {
    if (!isSupported) {
      alert("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ ghi √¢m");
      return;
    }
    const mimeType = getSupportedMimeType();
    if (!mimeType) {
      alert("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ c√°c codec audio c·∫ßn thi·∫øt");
      return;
    }
    if (!canPlayFormat(mimeType)) {
      console.warn("‚ö†Ô∏è Browser may not be able to play recorded format:", mimeType);
    }
    try {
      setIsProcessing(true);
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 48000,
          channelCount: 1,
          autoGainControl: true,
        },
      });
      streamRef.current = stream;
      await createMediaRecorder(stream, mimeType);
      setIsProcessing(false);
      console.log("üéôÔ∏è Continuous recording started with format:", mimeType);
    } catch (error) {
      console.error("L·ªói khi b·∫Øt ƒë·∫ßu ghi √¢m:", error);
      alert("Kh√¥ng th·ªÉ truy c·∫≠p microphone. Vui l√≤ng cho ph√©p quy·ªÅn truy c·∫≠p.");
      setIsProcessing(false);
    }
  };

  const stopRecording = () => {
    console.log("üîá Stopping continuous recording");
    setIsRecording(false);
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
      mediaRecorderRef.current.stop();
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }
  };

  const sendVoiceToServer = async (audioBlob) => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const arrayBuffer = reader.result;
        const uint8Array = new Uint8Array(arrayBuffer);
        if (socketService.socket && socketService.socket.connected) {
          const mimeType =
            mediaRecorderRef.current?.mimeType ||
            getSupportedMimeType() ||
            "audio/webm";
          const success = socketService.sendRefereeVoice(
            Array.from(uint8Array),
            mimeType
          );
          success ? resolve() : reject(new Error("Failed to send voice"));
        } else {
          reject(new Error("Socket not connected"));
        }
      };
      reader.onerror = () => reject(reader.error);
      reader.readAsArrayBuffer(audioBlob);
    });
  };

  const toggleRecording = () => {
    isRecording ? stopRecording() : startRecording();
  };

  return (
    <div className="p-4 space-y-4">
      <div className="flex justify-center">
        <button
          onClick={toggleRecording}
          disabled={isProcessing || !isSupported}
          className={`
            w-20 h-20 rounded-full flex items-center justify-center transition-all duration-300 transform
            ${isRecording
              ? "bg-red-500 hover:bg-red-600 animate-pulse scale-110"
              : "bg-blue-500 hover:bg-blue-600"}
            ${isProcessing ? "opacity-50 cursor-not-allowed" : "hover:scale-105"}
            text-white shadow-lg hover:shadow-xl
            ${!isSupported ? "bg-gray-400 cursor-not-allowed" : ""}
          `}
        >
          {isProcessing ? (
            <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-white"></div>
          ) : isRecording ? (
            <MicOff size={32} />
          ) : (
            <Mic size={32} />
          )}
        </button>
      </div>
      <div className="text-center">
        {isProcessing && (
          <p className="text-blue-600 font-medium">ƒêang kh·ªüi t·∫°o...</p>
        )}
        {isRecording && !isProcessing && (
          <p className="text-red-600 font-medium animate-pulse">
            üî¥ ƒêang thu √¢m
          </p>
        )}
        {!isRecording && !isProcessing && (
          <p className="text-gray-600">·∫§n mic ƒë·ªÉ b·∫Øt ƒë·∫ßu b√¨nh lu·∫≠n li√™n t·ª•c</p>
        )}
        {!isSupported && (
          <p className="text-red-600">Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ ghi √¢m</p>
        )}
      </div>
    </div>
  );
};

export default CommentarySection;
